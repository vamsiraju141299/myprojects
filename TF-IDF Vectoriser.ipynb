{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TF-IDF Vectoriser.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPJEL/TRhh/xaSpHBgiwaYa"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bgIqxOahlPqC","colab_type":"text"},"source":["Task-1"]},{"cell_type":"markdown","metadata":{"id":"b-jPdg-RlSDr","colab_type":"text"},"source":["Implementation using Sklearn"]},{"cell_type":"markdown","metadata":{"id":"foxRhdLslcxW","colab_type":"text"},"source":["Corpus"]},{"cell_type":"code","metadata":{"id":"cxWzbCEAlg0U","colab_type":"code","colab":{}},"source":["corpus = [\n","     'this is the first document',\n","     'this document is the second document',\n","     'and this is the third one',\n","     'is this the first document',\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gcy3kW7bl1yr","colab_type":"text"},"source":["sklearn Implementation"]},{"cell_type":"code","metadata":{"id":"H5vY8urll47X","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","vectorizer.fit(corpus)\n","skl_output = vectorizer.transform(corpus)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UcXQpRNDmCjT","colab_type":"text"},"source":["sklearn Feature Names in the given corpus after applying transform method. By Default they will be in the sorted order"]},{"cell_type":"code","metadata":{"id":"ueZ_ftudmWcV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"dc942023-a4c5-4b24-9982-3d9e5f65d2ea","executionInfo":{"status":"ok","timestamp":1589953298322,"user_tz":-330,"elapsed":2822,"user":{"displayName":"Vamsi Vemulamanda","photoUrl":"","userId":"10678927499212948648"}}},"source":["print(vectorizer.get_feature_names())"],"execution_count":4,"outputs":[{"output_type":"stream","text":["['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QSpA8qagmp4h","colab_type":"text"},"source":["shape of sklearn tfidf vectorizer output after applying transform method."]},{"cell_type":"code","metadata":{"id":"ShFGvbl1mrTL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ae5a3ded-175c-4acf-b1ea-6540b9151725","executionInfo":{"status":"ok","timestamp":1589953425646,"user_tz":-330,"elapsed":1425,"user":{"displayName":"Vamsi Vemulamanda","photoUrl":"","userId":"10678927499212948648"}}},"source":["skl_output.shape"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4, 9)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"21SRJ2ZHnSMo","colab_type":"text"},"source":["Here we will print the sklearn tfidf vectorizer idf values after applying the fit method"]},{"cell_type":"markdown","metadata":{"id":"z5xlswqNnWSz","colab_type":"text"},"source":[" After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value."]},{"cell_type":"code","metadata":{"id":"VkELCVE5nakB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"7667a00b-764b-48a5-addc-9f250aa962fe","executionInfo":{"status":"ok","timestamp":1589953599104,"user_tz":-330,"elapsed":1652,"user":{"displayName":"Vamsi Vemulamanda","photoUrl":"","userId":"10678927499212948648"}}},"source":["print(vectorizer.idf_)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n"," 1.         1.91629073 1.        ]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"msy24ybgoLhC","colab_type":"text"},"source":["sklearn tfidf values for first line of the above corpus."]},{"cell_type":"markdown","metadata":{"id":"n7MRynVioM5W","colab_type":"text"},"source":["Here the output is a sparse matrix"]},{"cell_type":"code","metadata":{"id":"I5QSc4JKohHK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"89a41256-901b-4157-c266-30cc25ba0937","executionInfo":{"status":"ok","timestamp":1589953845890,"user_tz":-330,"elapsed":1636,"user":{"displayName":"Vamsi Vemulamanda","photoUrl":"","userId":"10678927499212948648"}}},"source":["print(skl_output[0])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["  (0, 8)\t0.38408524091481483\n","  (0, 6)\t0.38408524091481483\n","  (0, 3)\t0.38408524091481483\n","  (0, 2)\t0.5802858236844359\n","  (0, 1)\t0.46979138557992045\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WnClYCXgo0SQ","colab_type":"text"},"source":["To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it."]},{"cell_type":"markdown","metadata":{"id":"_npfe5nio3x5","colab_type":"text"},"source":["This output is normalized using L2 normalization. sklearn does this by default."]},{"cell_type":"code","metadata":{"id":"oIctkoJqo9t5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"1b3c95ea-06ea-4a18-eb60-6e1bba9645b1","executionInfo":{"status":"ok","timestamp":1589953965950,"user_tz":-330,"elapsed":3690,"user":{"displayName":"Vamsi Vemulamanda","photoUrl":"","userId":"10678927499212948648"}}},"source":["print(skl_output[0].toarray())"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n","  0.38408524 0.         0.38408524]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S1Iq0olFpEid","colab_type":"text"},"source":["Custom Implementation"]},{"cell_type":"markdown","metadata":{"id":"UnQR4kvVpHfV","colab_type":"text"},"source":["Importing the libraries"]},{"cell_type":"code","metadata":{"id":"5NiAk3oCpMb7","colab_type":"code","colab":{}},"source":["from collections import Counter\n","from tqdm import tqdm\n","from scipy.sparse import csr_matrix\n","import math\n","import operator\n","from sklearn import preprocessing\n","import numpy\n","from sklearn.preprocessing import normalize"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YbIuVVBzps3g","colab_type":"code","colab":{}},"source":["def fit(dataset):\n","  unique_words = set() \n","  if isinstance(dataset, (list,)):\n","    for row in dataset:\n","       for word in row.split(\" \"):\n","         if len(word) < 2:\n","           continue\n","         unique_words.add(word)\n","    unique_words = sorted(list(unique_words))\n","    vocab = {j:i for i,j in enumerate(unique_words)}\n","    IDF_values=[]\n","    for i in vocab.keys():\n","      c=0\n","      for row in corpus:\n","        if i in row.split(\" \"):\n","          c+=1\n","      ans1=1+math.log((1+len(corpus))/(1+c))\n","      IDF_values.append(ans1)\n","    return vocab,IDF_values\n","  else:\n","     print(\"you need to pass list of sentance\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sSmiuRWArV4S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"3592679e-4b42-4b27-ab1b-3531e92af506","executionInfo":{"status":"ok","timestamp":1589968063495,"user_tz":-330,"elapsed":10948,"user":{"displayName":"Vamsi Vemulamanda","photoUrl":"","userId":"10678927499212948648"}}},"source":["vocab,IDF_values=fit(corpus)\n","print(\"vocab =\",vocab.keys())\n","print(\"IDF_values =\",IDF_values)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["vocab = dict_keys(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'])\n","IDF_values = [1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CNITY7nFsQW6","colab_type":"code","colab":{}},"source":["def transform(dataset,vocab):\n","    rows = []\n","    columns = []\n","    values = []\n","    if isinstance(dataset, (list,)):\n","        for idx, row in enumerate(tqdm(dataset)):\n","            word_freq = dict(Counter(row.split()))\n","            for word, freq in word_freq.items():             \n","                if len(word) < 2:\n","                    continue\n","                no_of_document_with_t=0\n","                for rowsp in dataset:\n","                  if word in rowsp.split(\" \"):\n","                    no_of_document_with_t=no_of_document_with_t+1\n","                ans=(freq/len(row.split()))*(math.log((1+len(corpus))/(no_of_document_with_t+1))+1)\n","                col_index = vocab.get(word, -1)\n","                if col_index !=-1:\n","                    rows.append(idx)\n","                    columns.append(col_index)\n","                    values.append(ans)\n","        nor= csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))\n","        X_normalized=preprocessing.normalize(nor.toarray(), norm='l2')\n","        return X_normalized\n","    else:\n","        print(\"you need to pass list of strings\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uIwuNkkLwtYQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"outputId":"5017dbb9-5aba-44d3-bc71-56013d077f34","executionInfo":{"status":"ok","timestamp":1589968422662,"user_tz":-330,"elapsed":3192,"user":{"displayName":"Vamsi Vemulamanda","photoUrl":"","userId":"10678927499212948648"}}},"source":["vocab,IDF_values = fit(corpus)\n","print()\n","print()\n","print(\"IDF_values : \",IDF_values)\n","print()\n","x=transform(corpus, vocab)\n","print(x)\n","print()\n","print(\"x shape: \",x.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["100%|██████████| 4/4 [00:00<00:00, 1422.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n","IDF_values :  [1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n","\n","[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n","  0.38408524 0.         0.38408524]\n"," [0.         0.6876236  0.         0.28108867 0.         0.53864762\n","  0.28108867 0.         0.28108867]\n"," [0.51184851 0.         0.         0.26710379 0.51184851 0.\n","  0.26710379 0.51184851 0.26710379]\n"," [0.         0.46979139 0.58028582 0.38408524 0.         0.\n","  0.38408524 0.         0.38408524]]\n","\n","x shape:  (4, 9)\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"TUJZe1V5gp8K","colab_type":"text"},"source":["Task 2"]},{"cell_type":"markdown","metadata":{"id":"yiwK33_pgxHy","colab_type":"text"},"source":["Loading the Document"]},{"cell_type":"code","metadata":{"id":"mnapTakIrRJL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a2488aac-869a-4b76-f228-46a829b6b6a5","executionInfo":{"status":"ok","timestamp":1589971372159,"user_tz":-330,"elapsed":3350,"user":{"displayName":"Vamsi Vemulamanda","photoUrl":"","userId":"10678927499212948648"}}},"source":["import pickle\n","with open('cleaned_strings', 'rb') as f:\n","    corpus = pickle.load(f)\n","    \n","# printing the length of the corpus loaded\n","print(\"Number of documents in corpus = \",len(corpus))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Number of documents in corpus =  746\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qAkcUIfqrRCU","colab_type":"code","colab":{}},"source":["def fit1(dataset):\n","  unique_words = set() \n","  if isinstance(dataset, (list,)):\n","    for row in dataset:\n","       for word in row.split(\" \"):\n","         if len(word) < 2:\n","           continue\n","         unique_words.add(word)\n","    unique_words = sorted(list(unique_words))\n","    vocab1 = {j:i for i,j in enumerate(unique_words)}\n","    IDF_values1=[]\n","    for i in vocab1.keys():\n","      c=0\n","      for row in corpus:\n","        if i in row.split(\" \"):\n","          c+=1\n","      ans1=1+math.log((1+len(corpus))/(1+c))\n","      IDF_values1.append(ans1)\n","    zip1=dict(zip(list(vocab1.keys()),IDF_values1))\n","    sorted1=sorted(zip1.items(),key= lambda x:x[1],reverse=True)\n","    sorted1=dict(sorted1[:50])\n","    unique_words=list(sorted1.keys())\n","    vocab1={j:i for i,j in enumerate(unique_words)}\n","    IDF_values1=list(sorted1.values())\n","\n","    return vocab1,IDF_values1\n","  else:\n","     print(\"you need to pass list of sentance\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tmikuxHIuWXG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"f9513702-6e37-44ce-cb7c-ca791a1688cb","executionInfo":{"status":"ok","timestamp":1589972606370,"user_tz":-330,"elapsed":3634,"user":{"displayName":"Vamsi Vemulamanda","photoUrl":"","userId":"10678927499212948648"}}},"source":["part2_vocab1,part2_IDF_values=fit1(corpus)\n","print(\"TOP 50 IDF values:\\n\",part2_IDF_values)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["TOP 50 IDF values:\n"," [6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RpCYZaXYvyFZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":867},"outputId":"e466116e-f2c8-414e-e7e8-9d32f85f9ce1","executionInfo":{"status":"ok","timestamp":1589972614156,"user_tz":-330,"elapsed":1428,"user":{"displayName":"Vamsi Vemulamanda","photoUrl":"","userId":"10678927499212948648"}}},"source":["part2_vocab1\n"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'aailiyah': 0,\n"," 'abandoned': 1,\n"," 'abroad': 2,\n"," 'abstruse': 3,\n"," 'academy': 4,\n"," 'accents': 5,\n"," 'accessible': 6,\n"," 'acclaimed': 7,\n"," 'accolades': 8,\n"," 'accurate': 9,\n"," 'accurately': 10,\n"," 'achille': 11,\n"," 'ackerman': 12,\n"," 'actions': 13,\n"," 'adams': 14,\n"," 'add': 15,\n"," 'added': 16,\n"," 'admins': 17,\n"," 'admiration': 18,\n"," 'admitted': 19,\n"," 'adrift': 20,\n"," 'adventure': 21,\n"," 'aesthetically': 22,\n"," 'affected': 23,\n"," 'affleck': 24,\n"," 'afternoon': 25,\n"," 'aged': 26,\n"," 'ages': 27,\n"," 'agree': 28,\n"," 'agreed': 29,\n"," 'aimless': 30,\n"," 'aired': 31,\n"," 'akasha': 32,\n"," 'akin': 33,\n"," 'alert': 34,\n"," 'alike': 35,\n"," 'allison': 36,\n"," 'allow': 37,\n"," 'allowing': 38,\n"," 'alongside': 39,\n"," 'amateurish': 40,\n"," 'amaze': 41,\n"," 'amazed': 42,\n"," 'amazingly': 43,\n"," 'amusing': 44,\n"," 'amust': 45,\n"," 'anatomist': 46,\n"," 'angel': 47,\n"," 'angela': 48,\n"," 'angelina': 49}"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"n9888SmFwYBS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"58be0e3e-11ef-47ec-80e2-d7d7885c6908","executionInfo":{"status":"ok","timestamp":1589972719926,"user_tz":-330,"elapsed":5286,"user":{"displayName":"Vamsi Vemulamanda","photoUrl":"","userId":"10678927499212948648"}}},"source":["a=transform(corpus,part2_vocab1)\n","print(a[0])\n"],"execution_count":39,"outputs":[{"output_type":"stream","text":["100%|██████████| 746/746 [00:04<00:00, 176.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0.]\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]}]}